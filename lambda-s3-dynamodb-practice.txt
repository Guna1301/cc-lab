# AWS Lambda Triggered by S3 Upload â€” Updates DynamoDB
# Author: guna

## ðŸ§© Goal
When a file is uploaded to an S3 bucket, a Lambda function is triggered.
It logs the file info into a DynamoDB table.

------------------------------------------------
STEP 1: Create DynamoDB Table
------------------------------------------------
Go to: AWS Console â†’ DynamoDB â†’ Create table

- Table name: FileUploads
- Partition key: FileName (String)
- Leave defaults â†’ Create table

------------------------------------------------
STEP 2: Create S3 Bucket
------------------------------------------------
Go to: AWS Console â†’ S3 â†’ Create bucket

- Bucket name: guna-file-upload-bucket (must be globally unique)
- Region: ap-south-1 (or your region)
- Uncheck: "Block all public access" (for testing only)
- Create bucket

------------------------------------------------
STEP 3: Create IAM Role for Lambda
------------------------------------------------
Go to: IAM â†’ Roles â†’ Create Role

- Trusted entity: AWS service â†’ Lambda
- Attach policies:
    - AmazonS3ReadOnlyAccess
    - AmazonDynamoDBFullAccess
- Role name: LambdaS3DynamoRole
- Create role

------------------------------------------------
STEP 4: Create Lambda Function
------------------------------------------------
Go to: Lambda â†’ Create Function

- Author from scratch
- Function name: S3ToDynamoFunction
- Runtime: Python 3.12
- Permissions: Use existing role â†’ LambdaS3DynamoRole
- Click: Create function

------------------------------------------------
STEP 5: Add S3 Trigger to Lambda
------------------------------------------------
In Lambda function page:

- Click: "Add Trigger"
- Choose: S3
- Bucket: guna-file-upload-bucket
- Event type: All object create events
- Suffix (optional): .txt
- Click: Add

------------------------------------------------
STEP 6: Lambda Code (Python)
------------------------------------------------
Paste this code in the Lambda code editor and click Deploy:

import boto3

dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table('FileUploads')

def lambda_handler(event, context):
    print("Event received:", event)
    
    for record in event['Records']:
        bucket = record['s3']['bucket']['name']
        key = record['s3']['object']['key']
        
        response = table.put_item(
            Item={
                'FileName': key,
                'Bucket': bucket
            }
        )
        print(f"Added {key} to DynamoDB.")
    
    return {
        'statusCode': 200,
        'body': 'File info stored in DynamoDB'
    }

------------------------------------------------
STEP 7: Test It â€” Upload a File to S3
------------------------------------------------
- Go to: S3 â†’ Your bucket
- Click: Upload â†’ Upload any file (e.g., test.txt)

Lambda will trigger automatically.

------------------------------------------------
STEP 8: Check DynamoDB Table
------------------------------------------------
- Go to: DynamoDB â†’ Tables â†’ FileUploads
- Click: Explore Table Items

You should see something like:

FileName        | Bucket  
----------------|----------------------------
test.txt        | guna-file-upload-bucket

------------------------------------------------
OPTIONAL CLEANUP
------------------------------------------------
- Delete S3 bucket and objects
- Delete Lambda function
- Delete DynamoDB table
- Delete IAM role

# End of tutorial
